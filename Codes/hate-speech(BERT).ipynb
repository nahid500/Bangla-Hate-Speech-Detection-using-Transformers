{"metadata":{"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8407735,"sourceType":"datasetVersion","datasetId":5003406}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets torch accelerate -U\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom datasets import load_dataset\n\ndataset = load_dataset('csv', data_files={'train': '/kaggle/input/hatespeech/train.csv', 'test': '/kaggle/input/hatespeech/val.csv'})\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T13:12:55.576175Z","iopub.execute_input":"2024-05-31T13:12:55.576887Z","iopub.status.idle":"2024-05-31T13:13:04.423335Z","shell.execute_reply.started":"2024-05-31T13:12:55.576849Z","shell.execute_reply":"2024-05-31T13:13:04.422419Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5952b9bef4d74a9dabad139e1d18080c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c7f479fe2cd462ebd2f3cba576e9f25"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Choose the model checkpoint (BERT or RoBERTa that supports Bangla, such as 'bert-base-multilingual-cased')\nmodel_checkpoint = \"bert-base-multilingual-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\ndef tokenize_function(examples):\n    # Tokenize the text\n    tokenized_inputs = tokenizer(examples['sentence'], padding=\"max_length\", truncation=True, max_length=512)\n    \n    # Include the labels\n    tokenized_inputs['labels'] = examples['hate speech']\n    \n    return tokenized_inputs\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T13:13:04.426693Z","iopub.execute_input":"2024-05-31T13:13:04.427144Z","iopub.status.idle":"2024-05-31T13:13:23.561819Z","shell.execute_reply.started":"2024-05-31T13:13:04.427117Z","shell.execute_reply":"2024-05-31T13:13:23.560872Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4ee9709b17c46f3a3cd42ff3f785ae3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9cda0f68ee2424e8ad55c7e0bb7852b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f50f927c02554fc29e889553514be08e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b430beb3a884320a122347124c6bccf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40224 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8b977ffcaae4bb7bf73d47d6e516833"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5028 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58e8d9adea7e4c0694e4f71babce2e1e"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nnum_labels = 2  # Binary classification\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T13:13:23.562850Z","iopub.execute_input":"2024-05-31T13:13:23.563116Z","iopub.status.idle":"2024-05-31T13:13:29.142661Z","shell.execute_reply.started":"2024-05-31T13:13:23.563092Z","shell.execute_reply":"2024-05-31T13:13:29.141715Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a33911ffc3df4742a1858b287438d749"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    save_total_limit=1,\n    report_to = 'none',\n    load_best_model_at_end=True,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T13:13:43.629027Z","iopub.execute_input":"2024-05-31T13:13:43.629899Z","iopub.status.idle":"2024-05-31T13:13:43.662040Z","shell.execute_reply.started":"2024-05-31T13:13:43.629863Z","shell.execute_reply":"2024-05-31T13:13:43.661126Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-05-31T13:13:29.277719Z","iopub.execute_input":"2024-05-31T13:13:29.278627Z","iopub.status.idle":"2024-05-31T13:13:29.283453Z","shell.execute_reply.started":"2024-05-31T13:13:29.278584Z","shell.execute_reply":"2024-05-31T13:13:29.282143Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from datasets import load_metric\n\naccuracy_metric = load_metric(\"accuracy\")\nprecision_metric = load_metric(\"precision\")\nrecall_metric = load_metric(\"recall\")\nf1_metric = load_metric(\"f1\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(-1)\n    \n    # Calculate all metrics\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n    precision = precision_metric.compute(predictions=predictions, references=labels, average='weighted')\n    recall = recall_metric.compute(predictions=predictions, references=labels, average='weighted')\n    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n    \n    return {\n        \"accuracy\": accuracy[\"accuracy\"],\n        \"precision\": precision[\"precision\"],\n        \"recall\": recall[\"recall\"],\n        \"f1\": f1[\"f1\"]\n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T13:13:29.284923Z","iopub.execute_input":"2024-05-31T13:13:29.285334Z","iopub.status.idle":"2024-05-31T13:13:31.223030Z","shell.execute_reply.started":"2024-05-31T13:13:29.285281Z","shell.execute_reply":"2024-05-31T13:13:31.222055Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3164243223.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n  accuracy_metric = load_metric(\"accuracy\")\n/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e45aab49b1f4d58a652f8925ebe110a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/precision/precision.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c121e05ad6274d85864c00fb0cae6b7b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/recall/recall.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41586a79024044eeb5ff8cf6f876d300"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f832b29611764f4a8aba9e9e1b83f3af"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Trainer, DefaultDataCollator\n\ndata_collator = DefaultDataCollator(return_tensors=\"pt\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['test'],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T13:14:01.200371Z","iopub.execute_input":"2024-05-31T13:14:01.201496Z","iopub.status.idle":"2024-05-31T16:16:26.590700Z","shell.execute_reply.started":"2024-05-31T13:14:01.201451Z","shell.execute_reply":"2024-05-31T16:16:26.589749Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2024-05-31 13:14:04.166860: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-31 13:14:04.166989: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-31 13:14:04.337814: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6285' max='6285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6285/6285 3:02:08, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.306400</td>\n      <td>0.266730</td>\n      <td>0.894391</td>\n      <td>0.894549</td>\n      <td>0.894391</td>\n      <td>0.894320</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.237300</td>\n      <td>0.268780</td>\n      <td>0.890215</td>\n      <td>0.892200</td>\n      <td>0.890215</td>\n      <td>0.890236</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.195800</td>\n      <td>0.280600</td>\n      <td>0.899165</td>\n      <td>0.899202</td>\n      <td>0.899165</td>\n      <td>0.899176</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.154400</td>\n      <td>0.319502</td>\n      <td>0.896181</td>\n      <td>0.896419</td>\n      <td>0.896181</td>\n      <td>0.896211</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.116200</td>\n      <td>0.380603</td>\n      <td>0.892403</td>\n      <td>0.892813</td>\n      <td>0.892403</td>\n      <td>0.892440</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6285, training_loss=0.20436715768648692, metrics={'train_runtime': 10932.4657, 'train_samples_per_second': 18.397, 'train_steps_per_second': 0.575, 'total_flos': 5.29168954540032e+16, 'train_loss': 0.20436715768648692, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"results = trainer.evaluate()\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T16:16:36.693937Z","iopub.execute_input":"2024-05-31T16:16:36.694633Z","iopub.status.idle":"2024-05-31T16:18:19.456597Z","shell.execute_reply.started":"2024-05-31T16:16:36.694597Z","shell.execute_reply":"2024-05-31T16:18:19.455646Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [158/158 01:41]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.2667298913002014, 'eval_accuracy': 0.8943914081145584, 'eval_precision': 0.8945492784441751, 'eval_recall': 0.8943914081145584, 'eval_f1': 0.8943203466102131, 'eval_runtime': 102.7539, 'eval_samples_per_second': 48.932, 'eval_steps_per_second': 1.538, 'epoch': 5.0}\n","output_type":"stream"}]}]}